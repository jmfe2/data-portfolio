# ğŸ›’ Supermarket Sales â€“ ETL and Data Validation Project

## ğŸ“Œ Project Overview

This project demonstrates a complete ETL (Extract, Transform, Load) pipeline using a real-world dataset of supermarket transactions. The goal is to extract sales data from a CSV file, transform and clean it using Python, and load it into a PostgreSQL database. We then validate the inserted data and prepare it for visualization in Power BI or other BI tools.

---

## ğŸš€ Highlights

- End-to-end ETL pipeline in Python  
- Data validation with SQLAlchemy  
- Clean export to CSV  
- Interactive dashboard built with Power BI  
- Version control and modular project structure  

---

### ğŸ” Insights

- Sales are relatively evenly distributed among the three cities: Yangon (33.8%), Mandalay (33.1%), and Naypyitaw (33.1%), with no dominant location.  
- Women generate a 5.3% higher average ticket than men (Female: \$322.99 vs. Male: \$306.63).  
- The most sold product line by quantity is **Electronic Accessories**, accounting for 19.2% of all units sold.  
- 61% of all sales occur between the 10th and 20th of each month, indicating a mid-month purchase pattern.  

---

## ğŸ§° Tech Stack

- **Python 3.11**
- **PostgreSQL** (local instance)
- **pandas** â€“ data manipulation
- **SQLAlchemy** â€“ database connection
- **dotenv** â€“ environment variable management
- **Power BI** â€“ for dashboards
- **Git + GitHub** â€“ version control

---

## ğŸ“ Project Structure

```bash
data-portfolio/
â”œâ”€â”€ data/02_supermarket_sales/       # Original and cleaned CSVs
â”œâ”€â”€ scripts/02_supermarket/          # ETL and cleaning scripts
â”œâ”€â”€ notebooks/02_supermarket_sales/  # Analysis notebooks (ETL, validation)
â”œâ”€â”€ figures/02_supermarket/          # Power BI visuals and .pbix file
â”œâ”€â”€ run_etl_supermarket.sh           # Executable Bash script to run the pipeline

```

Other folders in root project:

/data/02_supermarket_sales/ # Contains the original CSV file
/scripts/02_supermarket/ # Custom Python cleaning functions (e.g., cleaning.py)
/figures/02_supermarket/ # Visual outputs from data analysis and Power BI

ğŸ–¼ Sample dashboard preview:

![Power BI Dashboard](../../figures/02_supermarket/powerbi_supermarket.pbix)
![Power BI Dashboard Executive Summary](../../figures/02_supermarket/Executive_Summary_Powerbi.pdf)

---

## ğŸ“„ Dataset Description

The dataset used is [`supermarket_sales.csv`](https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales), which contains:

- Invoice details
- Branch and city
- Customer type and gender
- Product line and quantity
- Pricing, VAT, payment method
- Date and time of transaction

ğŸ“¦ Rows: 1,000  
ğŸ“Š Columns: 17

ğŸ§¼ After ETL, the cleaned version is exported to:

`/data/02_supermarket_sales/cleaned_data/supermarket_cleaned.csv`

---

## ğŸ“Š Power BI Dashboard

The Power BI report built for this project includes insights such as:

- Total sales by city
- Revenue per product line
- Average transaction by gender
- Sales by payment method and day

ğŸ”— The `.pbix` file is available in [`/figures/02_supermarket/powerbi_supermarket.pbix`](../figures/02_supermarket/powerbi_supermarket.pbix)

## ğŸ“¸ Power BI Visual Gallery

These screenshots provide a quick glance at the insights uncovered:

- **Sales Overview Dashboard**
  ![Sales Overview](../../figures/02_supermarket/porwer_bi_dashboard_all.jpeg)
- **Total Sales â€“ Yangon**
  ![Yangon Sales](../../figures/02_supermarket/power_b_yangon_city.jpeg)
- **Total Sales â€“ Mandalay**
  ![Mandalay Sales](../../figures/02_supermarket/power_bi_mandalay_city.jpeg)
- **Total Sales â€“ Naypyitaw**
  ![Naypyitaw Sales](../../figures/02_supermarket/power_bi_naypytaw_city.jpeg)

## âš™ï¸ How to Run

1. Clone the repo and set up a Python virtual environment (optional):

```bash
git clone https://github.com/jmfe2/data-portfolio.git
cd data-portfolio
```
   
2. Install libraries

```bash
pip install -r requirements.txt
```

3. Create a .env file in the root directory with the following keys 

*Make sure your `.env` file is not tracked by Git. A `.env.example` is suggested for collaborators.*

```bash
DB_USER=your_username
DB_PASS=your_password
DB_HOST=localhost
DB_PORT=5432
DB_NAME=your_database
```

## âš™ï¸ How to Run

After installing the requirements and setting up your `.env` file, you have two ways to run the ETL pipeline:

---

### ğŸ§© Option A â€“ Manual via Python
  
  From the root of the repository, run:
  ```bash
  python -m scripts.02_supermarket.etl_pipeline
  ```
  Or open the notebooks:
  - 02_supermarket_postgresql.ipynb â€“ Loads CSV into PostgreSQL
  - 03_validate_and_git.ipynb â€“ Runs validation queries and tracks changes with Git 

### âš™ï¸ Option B â€“ Run via Bash Script (macOS/Linux)

  1. Make the script executable (only once):
  ```bash
  chmod +x run_etl_supermarket.sh
  ```
  2. Then run the ETL pipeline:
  ```bash
  ./run_etl_supermarket.sh
  ```
  This script will:
  - Activate the virtual environment (.venv)
  - Load environment variables from .env
  - Run the full ETL process and log progress to /logs/etl_supermarket.log
  ğŸ“Œ Note: Make sure you're in the root directory (/data-portfolio) when running the script.

---

#### 4. Optional enhancements

- Mention log file location:

```md
ETL logs are written to: `/logs/etl_supermarket.log`

## Contact

ğŸ“¸ Instagram: @jmfloreslab
ğŸ’¼ LinkedIn: linkedin.com/in/jmfe

---

## âš–ï¸ License
This project is licensed under the  [MIT License](../../LICENSE). See the LICENSE file for details.